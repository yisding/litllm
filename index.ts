/**
 ğŸ”¥ litllm - the last LLM wrapper you'll see before you go to sleep at night
 ğŸ”¥ Get these MF'ing ğŸs out this MF'ing ğŸ”¥ -- SLJ -- MS -- YD
 ğŸ”¥ Step 1: npm i litllm
 ğŸ”¥ Step 2: ???
 ğŸ”¥ Step 3: get lit!
 */

import {
  ALL_AVAILABLE_LLAMADEUCE_MODELS,
  ALL_AVAILABLE_OPENAI_MODELS,
  Anthropic,
  ChatMessage,
  ChatResponse,
  LlamaDeuce,
  OpenAI,
} from "./node_modules/llamaindex/src/llm/LLM"; // Yes of course ğŸ”¥llm uses LITS

/**
 ğŸ”¥ Chat with a model 
 ğŸ”¥ @param model the LLM model
 ğŸ”¥ @param messages the messages to chat with
 ğŸ”¥ @param options additional model options like temperature, topP, and maxTokens
 ğŸ”¥ @returns the chat response
 */
export async function completion(
  model: string,
  messages: ChatMessage[],
  options: { temperature?: number; topP?: number; maxTokens?: number }
): Promise<ChatResponse> {
  if (model in ALL_AVAILABLE_OPENAI_MODELS) {
    return await new OpenAI({
      model: model as keyof typeof ALL_AVAILABLE_OPENAI_MODELS,
      ...options,
    }).chat(messages);
  } else if (model in ALL_AVAILABLE_LLAMADEUCE_MODELS) {
    return await new LlamaDeuce({
      model: model as keyof typeof ALL_AVAILABLE_LLAMADEUCE_MODELS,
      ...options,
    }).chat(messages);
  } else {
    return await new Anthropic({ model: model, ...options }).chat(messages);
  }
}
