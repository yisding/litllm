/**
 ğŸ”¥ litllm - the last LLM wrapper you'll see before you go to sleep at night
 ğŸ”¥ Get these MF'ing ğŸs out this MF'ing ğŸ”¥ -- SLJ -- MS -- YD
 ğŸ”¥ Step 1: npm i litllm
 ğŸ”¥ Step 2: ???
 ğŸ”¥ Step 3: get lit!
 */

import {
  ALL_AVAILABLE_ANTHROPIC_MODELS,
  ALL_AVAILABLE_LLAMADEUCE_MODELS,
  ALL_AVAILABLE_OPENAI_MODELS,
  Anthropic,
  ChatMessage,
  ChatResponse,
  LlamaDeuce,
  OpenAI,
} from "llamaindex"; // Yes of course ğŸ”¥llm uses LITS

/**
 ğŸ”¥ Checks if model is an OpenAI fine tuned model
 ğŸ”¥ @param model model name
 ğŸ”¥ @returns if the model matches the OpenAI fine tuning format
 */
export function isOpenAIFineTunedModel(model: string): boolean {
  return (
    model.startsWith("ft:") &&
    model.split(":")?.[1] in ALL_AVAILABLE_OPENAI_MODELS
  );
}

/**
 ğŸ”¥ Chat with a model 
 ğŸ”¥ @param model the LLM model
 ğŸ”¥ @param messages the messages to chat with
 ğŸ”¥ @param options additional model options like temperature, topP, and maxTokens
 ğŸ”¥ @returns the chat response
 */
export async function completion(
  model: string,
  messages: ChatMessage[],
  options: { temperature?: number; topP?: number; maxTokens?: number }
): Promise<ChatResponse> {
  if (model in ALL_AVAILABLE_OPENAI_MODELS || isOpenAIFineTunedModel(model)) {
    return await new OpenAI({
      model: model as keyof typeof ALL_AVAILABLE_OPENAI_MODELS,
      ...options,
    }).chat({ messages });
  } else if (model in ALL_AVAILABLE_LLAMADEUCE_MODELS) {
    return await new LlamaDeuce({
      model: model as keyof typeof ALL_AVAILABLE_LLAMADEUCE_MODELS,
      ...options,
    }).chat({ messages });
  } else if (model in ALL_AVAILABLE_ANTHROPIC_MODELS) {
    return await new Anthropic({
      model: model as keyof typeof ALL_AVAILABLE_ANTHROPIC_MODELS,
      ...options,
    }).chat({ messages });
  } else {
    throw new Error(
      `Model ${model} not found. Please check the model name and try again.`
    );
  }
}
