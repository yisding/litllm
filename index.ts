/**
 ğŸ”¥ litllm - the last LLM wrapper you'll see before you go to sleep at night
 ğŸ”¥ Get these MF'ing ğŸs out this MF'ing ğŸ”¥ -- SLJ -- MS -- YD
 ğŸ”¥ Step 1: npm i litllm
 ğŸ”¥ Step 2: npm i (openai|anthropic|replicate)
 ğŸ”¥ Step 3: get lit
 */

import {
  ALL_AVAILABLE_LLAMADEUCE_MODELS,
  ALL_AVAILABLE_OPENAI_MODELS,
  Anthropic,
  ChatMessage,
  ChatResponse,
  LlamaDeuce,
  OpenAI,
} from "./node_modules/llamaindex/src/llm/LLM"; // Yes of course ğŸ”¥llm uses LITS

export async function completion(
  model: string,
  messages: ChatMessage[],
  options: { temperature?: number; topP?: number; maxTokens?: number }
): Promise<ChatResponse> {
  if (model in ALL_AVAILABLE_OPENAI_MODELS) {
    return await new OpenAI({
      model: model as keyof typeof ALL_AVAILABLE_OPENAI_MODELS,
      ...options,
    }).chat(messages);
  } else if (model in ALL_AVAILABLE_LLAMADEUCE_MODELS) {
    return await new LlamaDeuce({
      model: model as keyof typeof ALL_AVAILABLE_LLAMADEUCE_MODELS,
      ...options,
    }).chat(messages);
  } else {
    return await new Anthropic({ model: model, ...options }).chat(messages);
  }
}
